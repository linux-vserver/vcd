\chapter{The Linux-VServer Project}
\label{ch:intro:vserver}


\marginpar{\medskip\includegraphics[scale=0.2]{intro/vserver-logo}}

The Linux-VServer project implements operating system-level virtualization
based on \textit{Security Contexts} which permit the creation of many
independent \textit{Virtual Private Servers} (VPS) that run simultaneously on a
single physical server at full speed, efficiently sharing hardware resources.

A VPS provides an almost identical operating environment as a conventional
Linux server. All services, such as ssh, mail, web and databases, can be
started on such a VPS, without (or in special cases with only minimal)
modification, just like on any real server.

Each virtual server has its own user account database and root password and is
isolated from other virtual servers, except for the fact that they share the
same hardware resources.


\section{Rationale}
\label{sec:intro:vserver:rationale}

Over the years, computers have become sufficiently powerful to use
virtualization to create the illusion of many smaller virtual machines, each
running a separate operating system instance.

There are several kinds of \textit{Virtual Machines} (VMs) providing similar
features, but differ in the degree of abstraction and the methods used for
virtualization.

Most of them accomplish what they do by emulating some real or fictional
hardware, which in turn consume real resources from the \textit{Host} (the
machine running the VMs). This approach, used by most system emulators, allows
the emulator to run an arbitrary operating systems, even for a different
hardware architecture. No modifications need to be made to the \textit{Guest}
(the operating system running in the VM) because it isn't aware of the fact
that it isn't running on real hardware.

Some system emulators require small modifications or specialized drivers to be
added to Host or Guest to improve performance and minimize the overhead
required for hardware emulation. Although this significantly improves
efficiency, there are still large amounts of resources being wasted in caches
and mediation between Guest and Host.

But suppose you do not want to run many different operating systems
simultaneously on a single box. Most applications running on a server do not
require hardware access or kernel level code, and could easily share a machine
with others, if they could be separated and secured...


\section{The Concept}
\label{sec:intro:vserver:concept}

At a basic level, a Linux server consists of three building blocks: hardware,
kernel and applications. The hardware usually depends on the provider or system
maintainer, and, while it has a big influence on the overall performance, it
cannot be changed that easily, and will likely differ from one setup to
another.

The main purpose of the kernel is to build an abstraction layer on top of the
hardware to allow processes (applications) to work with and operate on
resources without knowing the details of the underlying hardware. Ideally,
those processes would be completely hardware agnostic, by being written in an
interpreted language and therefore not requiring any hardware-specific
knowledge.

Given that a system has enough resources to drive ten times the number of
applications a single Linux server would usually require, why not put ten
servers on that box, which will then share the available resources in an
efficient manner?

Most server applications (e.g. httpd) will assume that it is the only
application providing a particular service, and usually will also assume a
certain filesystem layout and environment. This dictates that similar or
identical services running on the same physical server, but for example, only
differing in their addresses, have to be coordinated. This typically requires a
great deal of administrative work which can lead to reduced system stability
and security.

The basic concept of the Linux-VServer solution is to separate the user-space
environment into distinct units (Virtual Private Servers) in such a way that
each VPS looks and feels like a real server to the processes contained within.

Although different Linux distributions use (sometimes heavily) patched kernels
to provide special support for unusual hardware or extra functionality, most
Linux distributions are not tied to a special kernel.

Linux-VServer uses this fact to allow several distributions, to be run
simultaneously on a single, shared kernel, without direct access to the
hardware, and share the resources in a very efficient way.


\section{Usage Scenarios}
\label{sec:intro:vserver:usage}

The primary goal of this project is to create virtual servers sharing the same
machine. A virtual server operates like a normal Linux server. It runs normal
services such as telnet, mail servers, web servers, and database servers.


\subsection{Administrative Separation}

As the hardware evolves, it is tempting to put more and more tasks on a server.
Though Linux could reliably handle it, at some point, the system administrator
will likely end up with too much stuff and users on one system and worrying
about system updates. Additionally, separating different or similar services
which otherwise would interfere with each other, either because they are poorly
designed or because they are simply incapable of peaceful coexistence for
whatever reason, may often be complex or even impossible.

The Linux-VServer project addresses this issue. The same box is able to run
multiple virtual servers and each one does the job it is supposed to do. If the
administrator needs to upgrade to PHP 5 for a given project, he can update just
one virtual server and does not affect the others.

Also, the root password of a virtual servers can be given to foreign
administrators, thus allows him to perform updates, restart services or update
system configuration without having to know or worry about other virtual
servers hosted on the same machine. This allows a clever provider to sell
Virtual Private Servers, which uses less resources than other virtualization
techniques, which in turn allows to put more units on a single machine.

The list of providers doing so is relatively long, and so this is rightfully
considered the main area of application. See the Linux-VServer project
page\footnote{\url{http://linux-vserver.org/VServer_Hosting}} for a (possibly
incomplete) list of companies providing Virtual Private Servers based on the
Linux-VServer technology.


\subsection{Enhancing Security}

While it can be interesting to run several virtual servers in one box, there is
one concept potentially more generally useful. Imagine a physical server
running a single virtual server. The goal is to isolate the main environment
from any service, any network. You boot in the main environment, start very few
services and then continue in the virtual server.

The service in the main environment would be:

\begin{itemize}
	\item Unreachable from the network.

	\item Able to log messages from the virtual server in a secure way. The
		virtual server would be unable to change/erase the logs. Even a cracked
		virtual server would not be able the edit the log.

	\item Able to run intrusion detection facilities, potentially spying the
		state of the virtual server without being accessible or noticed. For
		example, tripwire could run there and it would be impossible to
		circumvent its operation or trick it.
\end{itemize}

Another option is to put the firewall in a virtual server, and pull in the DMZ,
containing each service in a separate VPS. On proper configuration, this setup
can reduce the number of required machines drastically, without impacting
performance.


\subsection{Resource Indepedence}

Since virtual servers are only guests on the hardware they are using, they are
not aware of the specifics: they do not contain disk configurations, kernels or
network configurations.

One key feature of a virtual server is the independence from the actual
hardware. Most hardware issues are irrelevant for a virtual server
installation.

The main server acts as a host and takes care of all the details. The virtual
server is just a client and ignores all the details. As such, the client can be
moved to another physical server with very few manipulations.

For example, to move the virtual server from one physical computer to another,
it sufficient to do the following:

\begin{itemize}
	\item shutdown the running virtual server
	\item copy it over to the other machine
	\item copy the configuration
	\item start the virtual server on the new machine
\end{itemize}

No adjustments to user setup, password database or hardware configuration are
required, as long as both machine architectures are binary compatible.

Thus, once a virtual server is using more resource than expected, the
administrator can easily move it to another machine without the need to worry
about configuration files for disk layout, network interfaces etc. A virtual
server is just a directory on the filesystem of host system.


\subsection{Distribution Independence}

People are often talking about their preferred distribution. Should one use
Fedora, Debian or something else? Should one give a spin to the latest and
greatest distribution just for the sake of it?

With virtual servers, the choice of a distribution is less important. When you
select a distribution, you expect it will do the following:

\begin{itemize}
	\item Good hardware support/detection
	\item Good package technology/updates
	\item Good package selection
	\item Reliable packages
\end{itemize}

The choice is important because every service running on a box will be using
the same distribution. Most distributions out there are good and reliable.
Still each one has its peculiarities and probably flaws. For example, one
distribution is doing a great job on security but is not delivering the latest
and greatest PHP. Now because you have decided to use this distribution for
some projects, using virtual servers does not prevent you from using another
distribution for other projects or even a second virtual server for existing
projects.


\subsection{Fail-over Scenarios}

Pushing the limit a little further, replication technology could be used to
keep an up-to-the-minute copy of the filesystem of a running virtual server.
This would permit a very fast fail-over if the running server goes offline for
whatever reason.

All the known methods to accomplish this, starting with network replication via
rsync, or drbd, via network devices, or shared disk arrays, to distributed
filesystems, can be utilized to reduce the down-time and improve overall
efficiency.


\subsection{Experimenting and Upgrading}

If the system administrator intends to upgrade a system to get new features or
security updates, he probably first wants to test new packages on a development
machine, before the production server can be updated. Under normal
circumstances, i.e. all test have been passed, the upgrade procedure will look
something like this:

\begin{itemize}
	\item Doing a backup of the server
	\item Perform all the upgrades and install the new applications
\end{itemize}

Two hours later something does not work as expected and -- to make it even
worse -- it works fine on the development machine. Every system administrator
has experienced this scenario at least once.

Another solution to this problem would be to install the new production server
on new hardware, but this is not always possible, due to lack of hardware or
the effort needed to clone an existing machine.

Using virtual servers, all this becomes very easy:

\begin{itemize}
	\item Stop the virtual server in production
	\item Make a copy of the virtual server
	\item Perform the upgrades in the new virtual server
\end{itemize}

To get back to the example from above, two hours later something does not work
as expected and there is no immediate fix for the problem.

Again, using virtual servers, the (temporary) solution to this problem is very
easy:

\begin{itemize}
	\item Stop the new virtual server and assign it a new IP address
	\item Start both the old and new virtual server
\end{itemize}

Now the old one is still online and the issue can be tracked down on your new
virtual server using a different IP address. After the problem has been fixed
the new virtual server will be reassign the old IP address and serve for
production.


% TODO: more info needed here
\subsection{Development and Testing}

Consider a software tool or package which should be built for several versions
of a specific distribution (Mandrake 8.2, 9.0, 9.1, 9.2, 10.0) or even for
different distributions.

This is easily solved with Linux-VServer. Given plenty of disk space, the
different distributions can be installed and running side by side, simplifying
the task of switching from one to another.

Of course this can be accomplished by chroot() alone, but with Linux-VServer
it's a much more realistic simulation.


\section{Features}

Recent Linux Kernels already provide many security features that are utilized
by Linux-VServer to do its work. Especially features such as the Linux
Capability System, Resource Limits, File Attributes and the Change Root
Environment. The following sections will give a short overview about each of
these and the additional features implemented by the Linux-VServer project.


\subsection{Capabilities and Flags}

A capability is a token used by a process to prove that it is allowed to
perform an operation on an object. The Linux Capability System is based on
"POSIX Capabilities", a somewhat different concept, designed to split up the
all powerful root privilege into a set of distinct privileges.

\subsubsection{POSIX Capabilities}

A process has three sets of bitmaps called the inheritable(I), permitted(P),
and effective(E) capabilities. Each capability is implemented as a bit in each
of these bitmaps that is either set or unset.

When a process tries to do a privileged operation, the operating system will
check the appropriate bit in the effective set of the process (instead of
checking whether the effective uid of the process is 0 as is normally done).

For example, when a process tries to set the clock, the Linux kernel will check
that the process has the \verb,CAP_SYS_TIME, bit (which is currently bit 25)
set in its effective set.

The permitted set of the process indicates the capabilities the process can
use. The process can have capabilities set in the permitted set that are not in
the effective set.

This indicates that the process has temporarily disabled this capability. A
process is allowed to set a bit in its effective set only if it is available in
the permitted set. The distinction between effective and permitted exists so
that processes can "bracket" operations that need privilege.

The inheritable capabilities are the capabilities of the current process that
should be inherited by a program executed by the current process. The permitted
set of a process is masked against the inheritable set during \verb,exec(),.
Nothing special happens during \verb,fork(), or \verb,clone(),. Child processes
and threads are given an exact copy of the capabilities of the parent process.

The implementation in Linux stopped at this point, whereas POSIX Capabilities
require the addition of capability sets to files too, to replace the SUID flag
(at least for executables).

\subsubsection{Upper Bound for Capabilities}

Because the current Linux Capability system does not implement the filesystem
related portions of POSIX Capabilities which would make setuid and setgid
executables secure, and because it is much safer to have a secure upper bound
for all processes within a context, an additional per-context capability mask
has been added to limit all processes belonging to that context to this mask.
The meaning of the individual caps (bits) of the capability bound mask is
exactly the same as with the permitted capability set. 

\subsubsection{Context Capabilities}

As the Linux capabilities have almost reached the maximum number that is
possible without heavy modifications to the kernel, it was a natural step to
add a context-specific capability system.

The Linux-VServer context capability set acts as a mechanism to fine tune
existing Linux capabilities. It is not visible to the processes within a
context, as they would not know how to modify or verify it. 

In general there are two ways to use those capabilities: 

\begin{itemize}
\item Require one or a number of context capabilities to be set in addition to
	a given Linux capability, each one controlling a distinct part of the
	functionality. For example the \verb,CAP_NET_ADMIN, could be split into RAW
	and PACKET sockets, so you could take away each of them separately by not
	providing the required context capability. 

\item Consider the context capability sufficient for a specified functionality,
	even if the Linux Capability says something different. For example mount()
	requires \verb,CAP_SYS_ADMIN, which adds a dozen other things we do not
	want, so we define \verb,VXC_SECURE_MOUNT, to allow mounts for certain
	contexts. 
\end{itemize}

The difference between the context flags and the context capabilities is more
an abstract logical separation than a functional one, because they are handled
very similar.
% TODO: add list of caps/flags to appendix and cross-ref


\subsection{Resource Limits}

Most properties related to system resources, might it be the memory
consumption, the number of processes or file-handles, qualify for imposing
limits on them. 

The Linux kernel provides the \verb,getrlimit, and \verb,setrlimit, system
calls to get and set resource limits per process. Each resource has an
associated soft and hard limit. The soft limit is the value that the kernel
enforces for the corresponding resource. The hard limit acts as a ceiling for
the soft limit: an unprivileged process may only set its soft limit to a value
in the range from 0 up to the hard limit, and (irreversibly) lower its hard
limit. A privileged process (one with the \verb,CAP_SYS_RESOURCE, capability)
may make arbitrary changes to either limit value. 

The Linux-VServer kernel extends this system to provide resource limits for
whole contexts, not just single processes. Additionally a few new limit types
missing in the vanilla kernel were introduced. 

Additionally the context limit system keeps track of observed maxima and
resource limit hits, to provide some feedback for the administrator.
% TODO: add list of limits to appendix and cross-ref


\subsection{CPU Scheduler}

It is important to have a decent understanding of both processes and threads
before learning about schedulers.

\subsubsection{Programs and Processes}

A program is a combination of instructions and data put together to perform a
task when executed. A process is an instance of a program (what one might call
a "running" program). An analogy is that programs are like classes in languages
like C++ and Java, and processes are like objects (instantiated instances of
classes). Processes are an abstraction created to embody the state of a program
during its execution. This means keeping track of the data that is associated
with a thread or threads of execution, which includes variables, hardware state
(e.g. registers and the program counter, etc...), and the contents of an
address space.

\subsubsection{Threads}

A process can have multiple threads of execution that work together to
accomplish its goals. These threads of execution are aptly named threads. A
kernel must keep track of each thread's stack and hardware state, or whatever
is necessary to track a single flow of execution within a process. Usually
threads share address spaces, but they do not have to (often they merely
overlap). It is important to remember that only one thread may be executing on
a CPU at any given time, which is basically the reason kernels have CPU
schedulers. An example of multiple threads within a process can be found in
most web browsers. Usually at least one thread exists to handle user interface
events (like stopping a page load), one thread exists to handle network
transactions, and one thread exists to render web pages.

\subsubsection{Scheduling in Linux}

Multitasking kernels (like Linux) allow more than one process to exist at any
given time, and furthermore each process is allowed to run as if it were the
only process on the system. Processes do not need to be aware of any other
processes unless they are explicitly designed to be. This makes programs easier
to develop, maintain, and port.

Though each CPU in a system can execute only one thread within a process at a
time, many threads from many processes appear to be executing at the same time.
This is because threads are scheduled to run for very short periods of time and
then other threads are given a chance to run. A kernel's scheduler enforces a
thread scheduling policy, including when, for how long, and in some cases where
(on Symmetric Multiprocessing (SMP) systems) threads can execute.

Normally the scheduler runs in its own thread, which is woken up by a timer
interrupt. Otherwise it is invoked via a system call or another kernel thread
that wishes to yield the CPU. A thread will be allowed to execute for a certain
amount of time, then a context switch to the scheduler thread will occur,
followed by another context switch to a thread of the scheduler's choice. This
cycle continues, and in this way a certain policy for CPU usage is carried out.

\subsubsection{Token Bucket Extension}

While the basic idea of Linux-VServer is a peaceful coexistence of all
contexts, sharing the common resources in a respectful way, it is sometimes
useful to control the resource distribution for resource hungry processes.

The basic principle of a Token Bucket is not very new. It is given here as an
example for the Hard CPU Limit. The same principle also applies to scheduler
priorities, network bandwidth limitation and resource control in general.

The Linux-VServer scheduler uses this mechanism in the following way: consider
a bucket of a certain size $S$ which is filled with a specified amount of
tokens $R$ every interval $T$, until the bucket is ``full'' -- excess tokens
are spilled. At each timer tick, a running process (here running means
actually needing the CPU as opposed to ``running'' as in ``existing'') consumes
exactly one token from the bucket, unless the bucket is empty, in which case
the process is put on a hold queue until the bucket has been refilled with a
minimum $M$ of tokens. The process is then rescheduled.

A major advantage of a Token Bucket is that a certain amount of tokens can be
accumulated in times of quiescence, which later can be used to burst when
resources are required.

Where a per-process Token Bucket would allow for a CPU resource limitation
of a single process, a Context Token Bucket allows to control the CPU usage
of all confined processes.

Another approach, which is also implemented, is to use the current fill
level of the bucket to adjust the process priority, thus reducing the
priority of processes belonging to excessive contexts.

\subsubsection{Hard CPU Limit}

The simplest configuration is to just give every context an upper bound for
CPU allocation. The important factor is the ratio:

\begin{center}
$$\frac{R}{T} \cdot 100 = \%\mbox{ CPU allocation}$$
\end{center}

Note that this is the proportion of a \emph{single} CPU in the system. So, if
there are four CPUs and a virtual server should get a complete CPU for itself,
then you would set $R=1, T=4$.

It is advantageous to smooth operation of the algorithm to make the interval as
small as possible (or much smaller than the bucket size). You can in most cases
simplify the fraction, such as changing \nicefrac{30}{100} to \nicefrac{3}{10}.

\subsubsection{Burst time}

To penalize processes after a certain amount of \emph{burst time}, i.e. putting
them on the hold queue, you can use the maximum size $S$ of the bucket and the
minimum number of tokens $M$ to \emph{hold} processes.

Consider a context with a limit of \nicefrac{1}{2} of CPU time, a bucket of
15000 tokens and a minimum of 2500 tokens. Given that the kernel timer runs at
1000Hz processes that have used the CPU for 30 seconds will be put on hold for
5 seconds. The following formula can be used to calculate $S$ and $M$, using
$B$ as burst time and $H$ as hold time:

\begin{eqnarray*}
M &=& H \cdot \mbox{Hz} \cdot \frac{R}{T}\\
S &=& B \cdot \mbox{Hz} \cdot \left (1 - \frac{R}{T} \right )
\end{eqnarray*}

\subsubsection{Guarantees}

A guarantee is nearly the same as a pure hard limit, except that you must not
allocate more than 100\% CPU time to all contexts. In other words, if you have
$N$ contexts and give each one a guarantee of more than \nicefrac{1}{N} CPU
time, it would result in more CPU time needed than physically available, which
cannot work out. The important factor here is the sum of all ratios:

$$\sum_{i=1}^N \frac{R_i}{T_i} \le 1$$


\subsubsection{Fair Share}

The fair share configuration is similar to guarantees, except that if the CPU
is idle a context can allocate more CPU time than its guarantee/limit. The
scheduler and bucket configuration was extended in Linux-VServer 2.1.1 to allow
fair share scheduling and is also know as \emph{idle time}.

Consider a configuration with 5 contexts each limited to \nicefrac{1}{5} of CPU
time, where two of these contexts run CPU intensive processes and the rest is
idle. Given that each context may only allocate \nicefrac{1}{5} of CPU time,
\nicefrac{3}{5} of CPU time are wasted since 3 contexts are idle.

To distribute the wasted CPU time \emph{fair} among contexts that could need it,
you can configure an allocation ratio for \nicefrac{R}{T} during idle times,
namely \nicefrac{$R^2$}{$T^2$}. To calculate the cpu distribution for context $k$
the following formula is used:

$$\left ( \frac{C \cdot \frac{R^2_k}{T^2_k}}{\sum_{i=1}^N \frac{R^2_i}{T^2_i}} +
\frac{R_k}{T_k} \right ) \cdot 100 = \%\mbox{ CPU allocation}$$

where $C$ is the idle CPU time, \nicefrac{3}{5} in our example. Consider a
\nicefrac{$R^2$}{$T^2$} ratio of \nicefrac{1}{2} for the first guest and
\nicefrac{1}{4} for the second. This would result in:

$$\left ( \frac{\frac{3}{5} \cdot \frac{1}{2}}{\frac{1}{2} + \frac{1}{4}} +
\frac{1}{5} \right ) \cdot 100 = 60\%\mbox{ CPU allocation for context 1}$$

$$\left ( \frac{\frac{3}{5} \cdot \frac{1}{4}}{\frac{1}{2} + \frac{1}{4}} +
\frac{1}{5} \right ) \cdot 100 = 40\%\mbox{ CPU allocation for context 2}$$

If the idle time ratio is the same for all contexts, the formula can be
simplified:

$$\left ( C \cdot \frac{1}{N} + \frac{R_k}{T_k} \right ) \cdot 100 = \%\mbox{
CPU allocation}$$

Therefore, if 3 of the above 5 contexts would run, i.e. \nicefrac{2}{5} of the
CPU are idle, and we have 3 running contexts, it would result in the expted
33\% split:

$$\left ( \frac{2}{5} \cdot \frac{1}{3} + \frac{1}{5} \right ) \cdot 100
\approx 33\%\mbox{ CPU allocation}$$


\subsection{Virtual Host Information}


\subsection{Filesystem Namespace}


\subsection{ProcFS Security}


\subsection{Chroot Barrier}


\subsection{Disk Limits}


\subsection{Quotas}


\subsection{Copy-on-Write Link Breaking}


\subsection{Network Addresses}


% TODO: more info needed here
\section{History}
\label{sec:intro:vserver:history}

\textbf{Jacques Gélinas} created the VServer project a number of years back.
He still does vserver development and the community can be glad to have him.
He's a genius, without him, Linux-VServer would not exist. Three cheers for
Jack.

But sometime during 2003 it became apparent that Jack didn't have the time to
keep vserver development up to pace. So in November, \textbf{Herbert Pötzl}
officially took charge of development. He now releases the vserver kernel
patches, announcing them on the vserver mailing list and making them available
for the public.

Additionally, \textbf{Enrico Scholz} decided to reimplement Jack's vserver
tools in \textbf{C}. These are now distributed as util-vserver. They are
backward compatible to Jack's tools as far as possible, but follow the kernel
patch development more closely.

In 2005, \textbf{Benedikt Böhm} started another reimplementation of the
userspace utilities, although with a completely different architecture in mind.
This new implementation is known as VServer Control Daemon as you already may
know when you read this manual \texttt{;-)}
